基于Scrapy框架的新浪微博爬虫
==========
#### 一个新浪微博的爬虫
####  a spider of sina weibo
## 项目说明
----  
    这是一个基于Python Scrapy框架的新浪微博爬虫      
项目依赖：  
===
    <em>Python3.7 MySQL5.7  Scrapy 1.5     

    这是一个通用的微博爬虫，目前暂时只实现了微博用户信息采集的功能，具体每一条微博的信息采集正在实现中    

    速度不够快，跑一个晚上大概也就10000条数据左右，没有实现代理ip，因为穷，所以买不起一天20+的代理ip，而且懒，所以不想自己去慢慢找能活得久一点的代理    

    本来做这个爬虫是为了大创的数据采集来用，最近刚开学事情不多所以会更新比较频繁，具体每一条微博的采集功能正在逐渐实现    

    第一次学习使用Scrapy框架，而且也算是第一次正式的用GitHub，前几天写的代码着实太愚蠢，但微博爬虫工作很繁琐，没有时间去修改之前写得太蠢的地方，准备一边继续写一边继续优化    

    现在大概爬了四万条左右的用户信息数据，主要爬取的网站是 [m.weibo](https://m.weibo.cn) 微博的移动站    

    相对PC站而言m站更容易获取数据，而且相对而言数据更加工整（ajax请求返回的json数据）更容易被提取出来，不需要进行模拟登陆，所以选择了对m站的数据进行爬取    

    数据库在我本地建好了已经，但是最近事情还是有点多，其实是我懒，所以我暂时先不把数据库表放上来（这坑以后会填）
  
    最后说一点数据库，数据的插入用到了Twisted框架，实现了将爬取到的数据异步插入，从而在某种程度上规避了通常意义上的同步插入给爬虫带来的运行阻塞
    
## 运行项目    
===  

    进入项目的目录    

    运行 scrapy crawl xxxspider    

    爬虫会自动运行爬取数据，但现在可能还暂时跑不了，因为项目前半部分对Scrapy框架还不熟悉就开始写了，着实写得有点蠢

## 项目的展望
===
#### 1. 速度的扩展  
    虽然嘴上说着穷，不过还是得把代理给搞定，没有那玩意爬取速度还是太慢了，不过问题不大，主要是没钱（留下了贫穷的泪水）在所有功能完善，bug改完之后，准备租一天的代理把代理给实现了，代理有爬取速度就上去了，说多了都是泪
#### 2. 分布式的展望
    完成单机上的爬虫之后，准备实现把这个爬虫扩展为分布式的爬虫，用Scrapy-Redis框架


